# A code for multiple logistic regression models with a sliding window approach to create class imbalance, augmentation with ROSE synthetic samples
# Load and balance the dataset with equal samples from both groups
# Initialize data structures and parameters for iterative model training
# Split data into train/test sets with a sliding window applied to Group A sizes
# Apply ROSE to balance training data with synthetic samples (except in first iteration)
# Train logistic regression model for each configuration
# Extract and save all model coefficients and top 10 per iteration
# Evaluate performance on the test set using confusion matrix and AUC
# Calculate classification metrics: accuracy, precision, recall, F1 score
# Save all performance metrics and coefficients to Excel files

library(dplyr)
library(tidyr)
library(openxlsx)
library(pROC)
library(ROSE)

# Load data (selected seed file)
data_full <- read.xlsx("dataseed219.xlsx")

# Balance the dataset by taking 600 samples from each group
data <- data_full %>% group_by(group) %>% slice_head(n = 600) %>% ungroup()

# Initialize empty dataframes to store results
all_results <- data.frame()
model_coefficients <- data.frame()
top10_coefficients <- data.frame()

# Configuration settings
initial_size <- 600        # Initial size of each group before splitting
step_size <- 3                   # Decrease in the Group A size for each config
num_configs <- 185         # Number of configurations/iterations
initial_test_A_size <- 200 # Starting test size for group A
initial_test_B_size <- 200 # Constant test size for group B
fixed_group_B_size <- 600       # Group B size remains constant throughout

# Get indices for groups A and B
group_A_indices <- which(data$group == "A")
group_B_indices <- which(data$group == "B")

# Loop through each configuration
for (idx in seq_len(num_configs)) {
    
    set.seed(123 + idx)  # Seed for reproducibility
    
    # Update group A test size
    group_A_test_size <- initial_test_A_size - (idx - 1)
    group_B_test_size <- initial_test_B_size
    
    # Sample training data for group A
    set.seed(456 + idx)
    group_A_train <- sample(group_A_indices, size = floor(2/3 * (initial_size - (idx - 1) * step_size)))
    
    # Define test data for group A
    remaining_A <- setdiff(group_A_indices, group_A_train)
    group_A_test <- if (length(remaining_A) < group_A_test_size) remaining_A else remaining_A[1:group_A_test_size]
    
    # Sample training and test data for group B
    group_B_subset <- group_B_indices[1:fixed_group_B_size]
    set.seed(789 + idx)
    group_B_train <- sample(group_B_subset, size = floor(2/3 * fixed_group_B_size))
    group_B_test <- setdiff(group_B_subset, group_B_train)[1:group_B_test_size]
    
    # Combine train and test sets
    train_indices <- c(group_A_train, group_B_train)
    test_indices <- c(group_A_test, group_B_test)
    train_data <- data[train_indices, ]
    test_data <- data[test_indices, ]
    
    # Convert outcome to numeric (for ROSE)
    train_data$group <- as.numeric(train_data$group == "B")
    test_data$group <- as.numeric(test_data$group == "B")
    
    # Apply ROSE to balance data for iterations beyond the first
    if (idx > 1) {
        num_synthetic_needed_A <- length(group_B_train)
        num_synthetic_needed_B <- length(group_B_train)
        
        total_synthetic_needed <- num_synthetic_needed_A + num_synthetic_needed_B
        rose_output <- ROSE(group ~ ., data = train_data, N = total_synthetic_needed * 2, p = 0.5, seed = sample.int(1e5, 1))$data
        rose_output$group <- as.numeric(rose_output$group)
        
        # Separate and select synthetic samples for each group
        rose_output_A <- rose_output[rose_output$group == 0, ]
        rose_output_B <- rose_output[rose_output$group == 1, ]
        rose_output_A_selected <- rose_output_A[sample(nrow(rose_output_A), min(nrow(rose_output_A), num_synthetic_needed_A)), ]
        rose_output_B_selected <- rose_output_B[sample(nrow(rose_output_B), min(nrow(rose_output_B), num_synthetic_needed_B)), ]
        
        train_data <- rbind(rose_output_A_selected, rose_output_B_selected)
    }
    
    # Convert group back to factor
    train_data$group <- factor(train_data$group, levels = c(0, 1), labels = c("A", "B"))
    test_data$group <- factor(test_data$group, levels = c(0, 1), labels = c("A", "B"))
    
    # Train logistic regression model
    logistic_model <- glm(group ~ ., data = train_data, family = binomial)
    
    # Store all model coefficients
    coef_df <- data.frame(Feature = names(coef(logistic_model)),
                          Coefficient = coef(logistic_model),
                          Iteration = idx)
    model_coefficients <- bind_rows(model_coefficients, coef_df)
    
    # Store top 10 absolute coefficients
    top10 <- coef_df %>% 
        filter(Feature != "(Intercept)") %>%
        arrange(desc(abs(Coefficient))) %>% 
        head(10)
    top10$Iteration <- idx
    top10_coefficients <- bind_rows(top10_coefficients, top10)
    
    # Predict on test set
    test_probabilities <- predict(logistic_model, newdata = test_data, type = "response")
    test_predictions <- ifelse(test_probabilities > 0.5, "B", "A")
    
    # Confusion matrix for test set
    test_conf_matrix <- table(Predicted = test_predictions, Actual = test_data$group)
    
    extract_conf_matrix_values <- function(conf_matrix) {
        AA <- ifelse("A" %in% rownames(conf_matrix) && "A" %in% colnames(conf_matrix), conf_matrix["A", "A"], 0)
        AB <- ifelse("B" %in% rownames(conf_matrix) && "A" %in% colnames(conf_matrix), conf_matrix["B", "A"], 0)
        BA <- ifelse("A" %in% rownames(conf_matrix) && "B" %in% colnames(conf_matrix), conf_matrix["A", "B"], 0)
        BB <- ifelse("B" %in% rownames(conf_matrix) && "B" %in% colnames(conf_matrix), conf_matrix["B", "B"], 0)
        return(c(AA, AB, BA, BB))
    }
    
    conf_values_test <- extract_conf_matrix_values(test_conf_matrix)
    
    # AUC for test set
    roc_obj <- roc(test_data$group, test_probabilities)
    auc_test <- as.numeric(auc(roc_obj))
    
    # Store test results
    iteration_results <- data.frame(
        iteration = idx,
        group_A_train = length(group_A_train),
        group_B_train = length(group_B_train),
        total_test_A = length(group_A_test),
        total_test_B = length(group_B_test),
        test_cm_A_A = conf_values_test[1],
        test_cm_A_B = conf_values_test[2],
        test_cm_B_A = conf_values_test[3],
        test_cm_B_B = conf_values_test[4],
        auc_test = auc_test
    )
    
    all_results <- bind_rows(all_results, iteration_results)
}

# Compute classification metrics on test set
all_results <- all_results %>%
    mutate(
        accuracy_test = (test_cm_A_A + test_cm_B_B) / (test_cm_A_A + test_cm_A_B + test_cm_B_A + test_cm_B_B),
        precision_test = test_cm_A_A / (test_cm_A_A + test_cm_B_A),
        recall_test = test_cm_A_A / (test_cm_A_A + test_cm_A_B),
        f1_score_test = 2 * (precision_test * recall_test) / (precision_test + recall_test),
        Percent_A_to_all = round(100 * (group_A_train + total_test_A) / 
                                     (group_A_train + group_B_train + total_test_A + total_test_B), 1),
        Percent_true_A = round(100 * test_cm_A_A / total_test_A, 1),
        Percent_true_B = round(100 * test_cm_B_B / total_test_B, 1)
    )

# Save results
write.xlsx(all_results, "LogReg_results_ROSE_completeAB.xlsx")
write.xlsx(model_coefficients, "LogReg_model_coefficients_ROSE_completeAB.xlsx")
write.xlsx(top10_coefficients, "LogReg_top10_coefficients_ROSE_completeAB.xlsx")

    
# plotting
library(dplyr)
library(openxlsx)
library(ggplot2)
library(gridExtra)
library(ggplotify)


all_results <- read.xlsx("LogReg_results_ROSE_completeAB.xlsx")
model_coefficients <- read.xlsx("LogReg_model_coefficients_ROSE_completeAB.xlsx")
top10_coefficients <- read.xlsx("LogReg_top10_coefficients_ROSE_completeAB.xlsx")

percent<-all_results[,c(1,24)]
colnames(percent)[1] <- "Iteration"

model_coefficients <- as.data.frame(full_join(model_coefficients, percent))

top10_coefficients <- as.data.frame(full_join(top10_coefficients, percent))


top10_coefficients_positive <- top10_coefficients[top10_coefficients$Coefficient > 0,]
top10_coefficients_negative <- top10_coefficients[top10_coefficients$Coefficient < 0,]

# lineplots
xxx <- all_results

p1 <- ggplot(xxx, aes(x = Percent_A_to_all)) +
    geom_line(aes(y = Percent_true_A, color = "A"),  size = 0.3) +
    geom_line(aes(y = Percent_true_B, color = "B"), size = 0.3) +
    scale_color_manual(
        values = c("A" = "red", "B" = "blue"),
        labels = c("Group A", "Group B")
    ) +
    labs(x = "Percent of Group A in Total Data", 
         y = "Percent Correctly Identified Samples", 
         color = NULL,  # Suppress legend title
         title = "Logistic Regression \nBoth Groups Replaced with ROSE Synthetic Samples") +
    theme_classic() +
    theme(plot.title = element_text(hjust = 0.5, size = 10)) +
    # Vertical dashed lines in purple that stop at Percent_true_A
    geom_segment(aes(x = 40, xend = 40, 
                     y = 0, 
                     yend = Percent_true_A[which.min(abs(Percent_A_to_all - 40))]), 
                 linetype = "dashed", color = "purple", size = 0.2) +
    geom_segment(aes(x = 45, xend = 45, 
                     y = 0, 
                     yend = Percent_true_A[which.min(abs(Percent_A_to_all - 45))]), 
                 linetype = "dashed", color = "purple", size = 0.2) +
    geom_segment(aes(x = 50, xend = 50, 
                     y = 0, 
                     yend = Percent_true_A[which.min(abs(Percent_A_to_all - 50))]), 
                 linetype = "dashed", color = "purple", size = 0.2) +
    # Horizontal dashed lines in purple from y-axis to the intersection points
    geom_segment(aes(x = 0, xend = 40, 
                     y = Percent_true_A[which.min(abs(Percent_A_to_all - 40))], 
                     yend = Percent_true_A[which.min(abs(Percent_A_to_all - 40))]),
                 linetype = "dashed", color = "purple", size = 0.2) +
    geom_segment(aes(x = 0, xend = 45, 
                     y = Percent_true_A[which.min(abs(Percent_A_to_all - 45))], 
                     yend = Percent_true_A[which.min(abs(Percent_A_to_all - 45))]),
                 linetype = "dashed", color = "purple", size = 0.2) +
    geom_segment(aes(x = 0, xend = 50, 
                     y = Percent_true_A[which.min(abs(Percent_A_to_all - 50))], 
                     yend = Percent_true_A[which.min(abs(Percent_A_to_all - 50))]),
                 linetype = "dashed", color = "purple", size = 0.2)


t1 <- ggplot(xxx, aes(x = Percent_A_to_all)) +
    geom_line(aes(y = accuracy_test, color = "accuracy_test"), size = 0.3) +
    geom_line(aes(y = precision_test, color = "precision_test"), size = 0.3) +
    geom_line(aes(y = recall_test, color = "recall_test"), size = 0.3) +
    geom_line(aes(y = f1_score_test, color = "f1_score_test"), size = 0.3) +
    geom_line(aes(y = auc_test, color = "auc_test"), , size = 0.3) +
    scale_color_manual(values = c("accuracy_test" = "blue", 
                                  "precision_test" = "red", 
                                  "recall_test" = "green", 
                                  "f1_score_test" = "purple", 
                                  "auc_test" = "orange"),
                       labels = c("Accuracy", "AUC", "f1 Score", "Precision", "Recall")
    ) +
    ylim(0, NA) +  
    theme_classic() +
    theme(plot.title = element_text(hjust = 0.5, size = 10)) +
    labs(x = "Percent of Group A in Total Data", 
         y = "", 
         color = NULL, 
         title = "Logistic Regression \nBoth Groups Replaced with ROSE Synthetic Samples") 

# Arrange the plots in a grid: 1 rows 2 columns
combined_plot <- grid.arrange(
    p1, t1,
    nrow = 1,
    ncol = 2
)

# Save the combined plot 
ggsave("LogReg_ROSE_GeneExp.png", combined_plot, width = 8.27, height = 3.6, units = "in")



# barplots of frequencies of features within the top 10

# Function to generate barplot with top 7 features and set x-axis limit to 185
plot_feature_frequency <- function(feature_data, title, low_color, high_color) {
    feature_counts <- as.data.frame(table(feature_data$Feature))
    colnames(feature_counts) <- c("Feature", "Count")
    feature_counts$Count <- as.numeric(feature_counts$Count)  # Convert Count to numeric
    
    # Keep only the top 7 features
    top7_features <- feature_counts %>%
        arrange(desc(Count)) %>%
        slice_head(n = 7)
    
    ggplot(top7_features, aes(x = reorder(Feature, Count), y = Count, fill = Count)) +
        geom_bar(stat = "identity") +
        coord_flip() +
        scale_fill_gradient(low = low_color, high = high_color) +
        scale_y_continuous(limits = c(0, 185)) +  # Apply limit after flipping
        theme_minimal() +
        labs(title = title, x = "Feature", y = "Count") +
        theme(axis.text.y = element_text(size = 7))
}

# Generate both barplots
p1 <- plot_feature_frequency(top10_coefficients_positive, 
                             "Top 7 Most Frequent Positive Features, Logistic Regression on Gene Expression Data \nBoth Groups Replaced with ROSE Synthetic Samples", 
                             low_color = "#FDE725", high_color = "#440154")

p2 <- plot_feature_frequency(top10_coefficients_negative, 
                             "Top 7 Most Frequent Negative Features, Logistic Regression on Gene Expression Data \nBoth Groups Replaced with ROSE Synthetic Samples", 
                             low_color = "#1f77b4", high_color = "#ff7f0e")

# Arrange and convert to ggplot object
combined_plot <- as.ggplot(grid.arrange(p1, p2, nrow = 2))

# Save the plot correctly
ggsave("LogReg_ROSE_GeneExp_feature_frequency_barplots_combined.png", combined_plot, width = 8.27, height = 3.6, dpi = 300, bg = "white")


# plot coefficients of top 4 most frequent features
# positive features
# remove outliers manually when necessary
# Count feature occurrences across all iterations
feature_counts <- table(unlist(top10_coefficients_positive$Feature))

# Select the top 4 most frequent features
top_features <- names(sort(feature_counts, decreasing = TRUE))[1:4]

# Filter data for the selected top features and ensure Percent_A_to_all >= 10%
filtered_data <- top10_coefficients_positive %>%
    filter(Feature %in% top_features, Percent_A_to_all >= 10) %>%
    arrange(desc(Percent_A_to_all))  # Reverse order

# Print the biggest coefficients for inspection
cat("Max Coefficients:\n")
print(head(filtered_data %>% arrange(-Coefficient), 5))  # Show 5 top coefficients

# Uncomment and modify this line to remove extreme values manually
filtered_data <- filtered_data %>% filter(Coefficient < 50)  # Adjust threshold as needed

# Define breaks to reduce the number of x-axis labels
break_positions <- unique(filtered_data$Percent_A_to_all)[seq(1, length(unique(filtered_data$Percent_A_to_all)), length.out = 10)]

# Plot the coefficients against Percent_A_to_all
ggplot(filtered_data, aes(x = Percent_A_to_all, y = Coefficient, color = Feature, group = Feature)) + 
    geom_line(size = 1) + 
    geom_point(size = 2) +
    scale_x_continuous(breaks = break_positions, limits = c(10, max(filtered_data$Percent_A_to_all))) +  # Start at 10%
    scale_color_manual(values = c("#1F77B4", "#17BECF", "#9D8FCE", "#6D6D76")) +  # Cooler, more subdued tones
    labs(x = "Percent of Group A in Total Data", y = "Coefficient", color = "Feature", title = "Top 4 Positive Features, Logistic Regression \nBoth Groups Replaced with ROSE Synthetic Samples") +
    theme_minimal() +
    theme(
        plot.title = element_text(hjust = 0.5, size = 20), 
        axis.text.x = element_text(angle = 90, hjust = 1),
        axis.title.x = element_text(size = 16),  # Increase size of x-axis label
        axis.title.y = element_text(size = 16),  # Increase size of y-axis label
        legend.title = element_text(size = 14),  # Increase size of legend title
        legend.text = element_text(size = 12)    # Increase size of legend text
) +
ylim(0, 9)
ggsave("LogReg_ROSE_GeneExp_top4_positive_coefficients_lineplot.png", width = 12, height = 5, dpi = 300, bg = "white")

# negative features
# Count feature occurrences across all iterations
feature_counts <- table(unlist(top10_coefficients_negative$Feature))

# Select the top 4 most frequent features
top_features <- names(sort(feature_counts, decreasing = TRUE))[1:4]

# Filter data for the selected top features and ensure Percent_A_to_all >= 10%
filtered_data <- top10_coefficients_negative %>%
    filter(Feature %in% top_features, Percent_A_to_all >= 10) %>%
    arrange(desc(Percent_A_to_all))  # Reverse order

# Print the lowest coefficients for inspection
cat("Min Coefficients:\n")
print(head(filtered_data %>% arrange(Coefficient), 5))  # Show 5 lowest coefficients

# Uncomment and modify this line to remove extreme values manually
# filtered_data <- filtered_data %>% filter(Coefficient > -50)  # Adjust threshold as needed

# Define breaks to reduce the number of x-axis labels
break_positions <- unique(filtered_data$Percent_A_to_all)[seq(1, length(unique(filtered_data$Percent_A_to_all)), length.out = 10)]

# Plot the coefficients against Percent_A_to_all
ggplot(filtered_data, aes(x = Percent_A_to_all, y = Coefficient, color = Feature, group = Feature)) + 
    geom_line(size = 1) + 
    geom_point(size = 2) +
    scale_x_continuous(breaks = break_positions, limits = c(10, max(filtered_data$Percent_A_to_all))) +  # Start at 10%
    scale_color_manual(values = c("#1F77B4", "#17BECF", "#9D8FCE", "#6D6D76")) +  # Cooler, more subdued tones
    labs(x = "Percent of Group A in Total Data", y = "Coefficient", color = "Feature", title = "Top 4 Negative Features, Logistic Regression, \nBoth Groups Replaced with ROSE Synthetic Samples") +
    theme_minimal() +
    theme(
        plot.title = element_text(hjust = 0.5, size = 20), 
        axis.text.x = element_text(angle = 90, hjust = 1),
        axis.title.x = element_text(size = 16),  # Increase size of x-axis label
        axis.title.y = element_text(size = 16),  # Increase size of y-axis label
        legend.title = element_text(size = 14),  # Increase size of legend title
        legend.text = element_text(size = 12)    # Increase size of legend text
) +
    ylim(-10, 0)

# Save the plot
ggsave("LogReg_ROSE_GeneExp_top4_negative_coefficients_lineplot.png", width = 12, height = 5, dpi = 300, bg = "white")
